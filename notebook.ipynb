{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18bc8be6",
   "metadata": {},
   "source": [
    "NASA SPACE APPS HACKATHON - PROJECT NOTEBOOK\n",
    "\n",
    "Title : Exoplanet Detection using Artificial Intelligence and Machine Learning\n",
    "\n",
    "Team : Ravenhart Vexmoor \n",
    "\n",
    "Track  : Best use of Science / Technology / Data / Global Impact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff682c25",
   "metadata": {},
   "source": [
    "1: Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efdf74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import astropy\n",
    "import lightkurve\n",
    "import matplotlib\n",
    "import tensorflow\n",
    "import shap\n",
    "import os, json\n",
    "import glob\n",
    "import numpy as np\n",
    "from astropy.timeseries import BoxLeastSquares\n",
    "from scipy.signal import periodogram\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import joblib\n",
    "from clean import load_and_preprocess\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import argparse\n",
    "from fastapi import FastAPI, UploadFile, File\n",
    "from preprocess import prepare_lightcurve_from_df\n",
    "from features import extract_features\n",
    "from vetting import vet_flags\n",
    "from models import build_baseline_model, FEATURE_COLUMNS\n",
    "from evaluate import evaluate_and_save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae9f9f",
   "metadata": {},
   "source": [
    "2: Loading and Processing of Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b4fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean.py\n",
    "\n",
    "def load_and_preprocess(file_path=\"dataset.csv\"):\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Drop duplicates (if any)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Handle missing values: fill with mean or drop (choose wisely)\n",
    "    df.fillna(df.mean(numeric_only=True), inplace=True)\n",
    "\n",
    "    # Select useful features for ML\n",
    "    features = [\n",
    "        \"teff\", \"logg\", \"feh\", \"radius\", \"mass\", \"dens\", \"kepmag\"\n",
    "    ]\n",
    "    target = \"nconfp\"  # Number of confirmed planets (can be binary label)\n",
    "\n",
    "    # Keep only selected columns if present\n",
    "    available_features = [f for f in features if f in df.columns]\n",
    "    if target in df.columns:\n",
    "        df = df[available_features + [target]]\n",
    "    else:\n",
    "        df = df[available_features]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a47c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess.py\n",
    "\n",
    "DATASET_PATH = \"datasets/\"\n",
    "OUTPUT_FILE = \"dataset.csv\"\n",
    "\n",
    "def load_and_merge_datasets(path=DATASET_PATH):\n",
    "    # Collect all CSV files in datasets/\n",
    "    csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "    print(f\"Found {len(csv_files)} CSV files.\")\n",
    "\n",
    "    dataframes = []\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file, low_memory=False)\n",
    "            print(f\"Loaded {file} with shape {df.shape}\")\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Could not load {file}: {e}\")\n",
    "\n",
    "    # Merge datasets (only common columns kept)\n",
    "    merged_df = pd.concat(dataframes, axis=0, ignore_index=True, sort=False)\n",
    "\n",
    "    print(f\"Total merged shape: {merged_df.shape}\")\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Drop duplicate rows\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Drop completely empty columns\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "    # Fill missing values with median (numeric) or mode (categorical)\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in [\"float64\", \"int64\"]:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "        else:\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_dataset(df, filename=OUTPUT_FILE):\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"✅ Processed dataset saved to {filename}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merged = load_and_merge_datasets()\n",
    "    processed = preprocess_data(merged)\n",
    "    save_dataset(processed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39105ae",
   "metadata": {},
   "source": [
    "3: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3c90f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "features.py\n",
    "Compute transit-search features using Box Least Squares (BLS) and other heuristics.\n",
    "Designed to produce tabular features for lightweight ML (RandomForest).\n",
    "\"\"\"\n",
    "\n",
    "def compute_bls(time, flux, min_period=0.3, max_period=30.0, n_periods=2000, q_min=0.005, q_max=0.1):\n",
    "    # time: days, flux: relative (around 0)\n",
    "    try:\n",
    "        model = BoxLeastSquares(time, flux)\n",
    "        periods = np.linspace(min_period, max_period, n_periods)\n",
    "        durations = np.linspace(q_min, q_max, 10) * periods[:, None]\n",
    "        power = model.power(periods, durations)\n",
    "        idx = np.nanargmax(power.power)\n",
    "        best_period = power.period[idx]\n",
    "        # select corresponding best duration & t0 (closest index)\n",
    "        best_duration = power.duration[idx]\n",
    "        best_t0 = power.transit_time[idx]\n",
    "        # depth approximate\n",
    "        phase_mask = ((time - best_t0 + 0.5*best_period) % best_period) / best_period\n",
    "        in_transit = (phase_mask > 0.5 - best_duration/(2*best_period)) & (phase_mask < 0.5 + best_duration/(2*best_period))\n",
    "        depth = np.nanmedian(flux[in_transit]) - np.nanmedian(flux[~in_transit]) if np.sum(in_transit) > 0 else 0.0\n",
    "        snr = power.power[idx] / (np.nanstd(flux) + 1e-9)\n",
    "        return {\"period\": float(best_period), \"duration\": float(best_duration), \"t0\": float(best_t0),\n",
    "                \"depth\": float(abs(depth)), \"snr\": float(snr)}, power\n",
    "    except Exception:\n",
    "        return {\"period\": np.nan, \"duration\": np.nan, \"t0\": np.nan, \"depth\": np.nan, \"snr\": np.nan}, None\n",
    "\n",
    "def transit_shape_metric(time, flux, period, t0, duration):\n",
    "    if np.isnan(period) or np.isnan(duration):\n",
    "        return np.nan\n",
    "    phase = ((time - t0 + 0.5*period) % period) / period\n",
    "    in_transit = (phase > 0.5 - duration/(2*period)) & (phase < 0.5 + duration/(2*period))\n",
    "    if np.sum(in_transit) < 5:\n",
    "        return np.nan\n",
    "    depth = np.nanmedian(flux[in_transit]) - np.nanmedian(flux[~in_transit])\n",
    "    edges = np.gradient(flux.astype(float))\n",
    "    slope = np.nanmedian(np.abs(edges[in_transit]))\n",
    "    return np.abs(slope) / (abs(depth) + 1e-9)\n",
    "\n",
    "def odd_even_depth_diff(time, flux, period, t0, duration):\n",
    "    if np.isnan(period):\n",
    "        return np.nan\n",
    "    phase = ((time - t0) % period) / period\n",
    "    in_transit = (phase < duration/period) | (phase > 1 - duration/period)\n",
    "    epochs = np.floor((time - t0) / period + 0.5).astype(int)\n",
    "    odd = in_transit & (epochs % 2 != 0)\n",
    "    even = in_transit & (epochs % 2 == 0)\n",
    "    try:\n",
    "        d_odd = np.nanmedian(flux[odd]) - np.nanmedian(flux[~odd]) if np.sum(odd) > 0 else 0.0\n",
    "        d_even = np.nanmedian(flux[even]) - np.nanmedian(flux[~even]) if np.sum(even) > 0 else 0.0\n",
    "        return abs(d_odd - d_even)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def secondary_eclipse_signal(time, flux, period, t0, duration):\n",
    "    if np.isnan(period):\n",
    "        return np.nan\n",
    "    t0_sec = t0 + 0.5 * period\n",
    "    phase = ((time - t0_sec) % period) / period\n",
    "    in_sec = (phase < duration/period) | (phase > 1 - duration/period)\n",
    "    if np.sum(in_sec) < 3:\n",
    "        return 0.0\n",
    "    return np.nanmedian(flux[in_sec]) - np.nanmedian(flux[~in_sec])\n",
    "\n",
    "def periodicity_features(time, flux):\n",
    "    try:\n",
    "        fs = 1.0 / np.median(np.diff(time))\n",
    "        f, pxx = periodogram(flux, fs=fs)\n",
    "        pxx = pxx / (np.sum(pxx) + 1e-12)\n",
    "        peak_power = float(np.nanmax(pxx))\n",
    "        entropy = -float(np.nansum(pxx * np.log(pxx + 1e-12)))\n",
    "        return {\"psd_peak\": peak_power, \"psd_entropy\": entropy}\n",
    "    except Exception:\n",
    "        return {\"psd_peak\": np.nan, \"psd_entropy\": np.nan}\n",
    "\n",
    "def extract_features(time, flux):\n",
    "    feat = {}\n",
    "    bls, power = compute_bls(time, flux)\n",
    "    feat.update(bls)\n",
    "    feat[\"shape_metric\"] = transit_shape_metric(time, flux, feat[\"period\"], feat[\"t0\"], feat[\"duration\"])\n",
    "    feat[\"odd_even_diff\"] = odd_even_depth_diff(time, flux, feat[\"period\"], feat[\"t0\"], feat[\"duration\"])\n",
    "    feat[\"secondary_depth\"] = secondary_eclipse_signal(time, flux, feat[\"period\"], feat[\"t0\"], feat[\"duration\"])\n",
    "    feat.update(periodicity_features(time, flux))\n",
    "    return feat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233ad5ee",
   "metadata": {},
   "source": [
    "5: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4757a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_xgb.py\n",
    "\n",
    "# -----------------------------\n",
    "# 1️ Load and preprocess dataset\n",
    "# -----------------------------\n",
    "df = load_and_preprocess(\"dataset.csv\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2️ Binary target: 1 if any planet, 0 if none\n",
    "# -----------------------------\n",
    "df[\"has_planet\"] = df[\"nconfp\"].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# -----------------------------\n",
    "# 3️ Features and target\n",
    "# -----------------------------\n",
    "feature_columns = df.drop(columns=[\"nconfp\", \"has_planet\"]).columns.tolist()\n",
    "X = df[feature_columns]\n",
    "y = df[\"has_planet\"]\n",
    "\n",
    "# Save feature columns for API\n",
    "joblib.dump(feature_columns, \"feature_columns.pkl\")\n",
    "print(\"✅ feature_columns.pkl saved\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4️ Train/test split (stratified)\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 5️ Train XGBoost classifier\n",
    "# -----------------------------\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    scale_pos_weight=(len(y_train)-y_train.sum())/y_train.sum(),  # handle imbalance\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\"\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# -----------------------------\n",
    "# 6️ Calibrate probabilities\n",
    "# -----------------------------\n",
    "calibrated_model = CalibratedClassifierCV(xgb_model, method='isotonic')\n",
    "calibrated_model.fit(X_train, y_train)\n",
    "\n",
    "# -----------------------------\n",
    "# 7️ Evaluate\n",
    "# -----------------------------\n",
    "y_pred = calibrated_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# -----------------------------\n",
    "# 8️ Save model\n",
    "# -----------------------------\n",
    "joblib.dump(calibrated_model, \"exoplanet_model_xgb.pkl\")\n",
    "print(\"✅ Model saved as exoplanet_model_xgb.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60758242",
   "metadata": {},
   "source": [
    "To train the class imbalances, the following code was executed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0f37e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "# -----------------------------\n",
    "# 1️ Load and preprocess dataset\n",
    "# -----------------------------\n",
    "df = load_and_preprocess(\"dataset.csv\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2️ Binary target: 1 if any planet, 0 if none\n",
    "# -----------------------------\n",
    "df[\"has_planet\"] = df[\"nconfp\"].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# -----------------------------\n",
    "# 3️ Features and target\n",
    "# -----------------------------\n",
    "feature_columns = df.drop(columns=[\"nconfp\", \"has_planet\"]).columns.tolist()\n",
    "X = df[feature_columns]\n",
    "y = df[\"has_planet\"]\n",
    "\n",
    "# Save feature columns for FastAPI\n",
    "joblib.dump(feature_columns, \"feature_columns.pkl\")\n",
    "print(\"✅ feature_columns.pkl saved\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4️ Train/test split (stratified)\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 5️ Apply SMOTE oversampling\n",
    "# -----------------------------\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "print(f\"✅ Training data balanced: {y_train_res.value_counts().to_dict()}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6️ Train RandomForest with balanced class weight\n",
    "# -----------------------------\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# -----------------------------\n",
    "# 7️ Evaluate\n",
    "# -----------------------------\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# -----------------------------\n",
    "# 8️ Save model\n",
    "# -----------------------------\n",
    "joblib.dump(model, \"exoplanet_model.pkl\")\n",
    "print(\"✅ Model saved as exoplanet_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4343c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "features.py\n",
    "Compute transit-search features using Box Least Squares (BLS) and other heuristics.\n",
    "Designed to produce tabular features for lightweight ML (RandomForest).\n",
    "\"\"\"\n",
    "\n",
    "def compute_bls(time, flux, min_period=0.3, max_period=30.0, n_periods=2000, q_min=0.005, q_max=0.1):\n",
    "    # time: days, flux: relative (around 0)\n",
    "    try:\n",
    "        model = BoxLeastSquares(time, flux)\n",
    "        periods = np.linspace(min_period, max_period, n_periods)\n",
    "        durations = np.linspace(q_min, q_max, 10) * periods[:, None]\n",
    "        power = model.power(periods, durations)\n",
    "        idx = np.nanargmax(power.power)\n",
    "        best_period = power.period[idx]\n",
    "        # select corresponding best duration & t0 (closest index)\n",
    "        best_duration = power.duration[idx]\n",
    "        best_t0 = power.transit_time[idx]\n",
    "        # depth approximate\n",
    "        phase_mask = ((time - best_t0 + 0.5*best_period) % best_period) / best_period\n",
    "        in_transit = (phase_mask > 0.5 - best_duration/(2*best_period)) & (phase_mask < 0.5 + best_duration/(2*best_period))\n",
    "        depth = np.nanmedian(flux[in_transit]) - np.nanmedian(flux[~in_transit]) if np.sum(in_transit) > 0 else 0.0\n",
    "        snr = power.power[idx] / (np.nanstd(flux) + 1e-9)\n",
    "        return {\"period\": float(best_period), \"duration\": float(best_duration), \"t0\": float(best_t0),\n",
    "                \"depth\": float(abs(depth)), \"snr\": float(snr)}, power\n",
    "    except Exception:\n",
    "        return {\"period\": np.nan, \"duration\": np.nan, \"t0\": np.nan, \"depth\": np.nan, \"snr\": np.nan}, None\n",
    "\n",
    "def transit_shape_metric(time, flux, period, t0, duration):\n",
    "    if np.isnan(period) or np.isnan(duration):\n",
    "        return np.nan\n",
    "    phase = ((time - t0 + 0.5*period) % period) / period\n",
    "    in_transit = (phase > 0.5 - duration/(2*period)) & (phase < 0.5 + duration/(2*period))\n",
    "    if np.sum(in_transit) < 5:\n",
    "        return np.nan\n",
    "    depth = np.nanmedian(flux[in_transit]) - np.nanmedian(flux[~in_transit])\n",
    "    edges = np.gradient(flux.astype(float))\n",
    "    slope = np.nanmedian(np.abs(edges[in_transit]))\n",
    "    return np.abs(slope) / (abs(depth) + 1e-9)\n",
    "\n",
    "def odd_even_depth_diff(time, flux, period, t0, duration):\n",
    "    if np.isnan(period):\n",
    "        return np.nan\n",
    "    phase = ((time - t0) % period) / period\n",
    "    in_transit = (phase < duration/period) | (phase > 1 - duration/period)\n",
    "    epochs = np.floor((time - t0) / period + 0.5).astype(int)\n",
    "    odd = in_transit & (epochs % 2 != 0)\n",
    "    even = in_transit & (epochs % 2 == 0)\n",
    "    try:\n",
    "        d_odd = np.nanmedian(flux[odd]) - np.nanmedian(flux[~odd]) if np.sum(odd) > 0 else 0.0\n",
    "        d_even = np.nanmedian(flux[even]) - np.nanmedian(flux[~even]) if np.sum(even) > 0 else 0.0\n",
    "        return abs(d_odd - d_even)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def secondary_eclipse_signal(time, flux, period, t0, duration):\n",
    "    if np.isnan(period):\n",
    "        return np.nan\n",
    "    t0_sec = t0 + 0.5 * period\n",
    "    phase = ((time - t0_sec) % period) / period\n",
    "    in_sec = (phase < duration/period) | (phase > 1 - duration/period)\n",
    "    if np.sum(in_sec) < 3:\n",
    "        return 0.0\n",
    "    return np.nanmedian(flux[in_sec]) - np.nanmedian(flux[~in_sec])\n",
    "\n",
    "def periodicity_features(time, flux):\n",
    "    try:\n",
    "        fs = 1.0 / np.median(np.diff(time))\n",
    "        f, pxx = periodogram(flux, fs=fs)\n",
    "        pxx = pxx / (np.sum(pxx) + 1e-12)\n",
    "        peak_power = float(np.nanmax(pxx))\n",
    "        entropy = -float(np.nansum(pxx * np.log(pxx + 1e-12)))\n",
    "        return {\"psd_peak\": peak_power, \"psd_entropy\": entropy}\n",
    "    except Exception:\n",
    "        return {\"psd_peak\": np.nan, \"psd_entropy\": np.nan}\n",
    "\n",
    "def extract_features(time, flux):\n",
    "    feat = {}\n",
    "    bls, power = compute_bls(time, flux)\n",
    "    feat.update(bls)\n",
    "    feat[\"shape_metric\"] = transit_shape_metric(time, flux, feat[\"period\"], feat[\"t0\"], feat[\"duration\"])\n",
    "    feat[\"odd_even_diff\"] = odd_even_depth_diff(time, flux, feat[\"period\"], feat[\"t0\"], feat[\"duration\"])\n",
    "    feat[\"secondary_depth\"] = secondary_eclipse_signal(time, flux, feat[\"period\"], feat[\"t0\"], feat[\"duration\"])\n",
    "    feat.update(periodicity_features(time, flux))\n",
    "    return feat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695e2460",
   "metadata": {},
   "source": [
    "7: Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e207f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "vetting.py\n",
    "Use features to create human-interpretable vetting flags:\n",
    "- odd-even mismatch -> eclipsing binaries\n",
    "- secondary eclipse -> eclipsing binaries or self-luminous companion\n",
    "- v-shape metric -> grazing/stellar eclipse\n",
    "- variability domination -> star spots/ pulsations\n",
    "- short single events -> moving objects (meteor/asteroid)\n",
    "\"\"\"\n",
    "\n",
    "def vet_flags(feat):\n",
    "    flags = {}\n",
    "    depth = abs(feat.get(\"depth\", 0.0) + 1e-9)\n",
    "    # Eclipsing binary indicators\n",
    "    flags[\"odd_even_flag\"] = (feat.get(\"odd_even_diff\", 0.0) > 0.1 * depth)\n",
    "    flags[\"secondary_flag\"] = (abs(feat.get(\"secondary_depth\", 0.0)) > 0.3 * depth)\n",
    "    flags[\"v_shape_flag\"] = (feat.get(\"shape_metric\", np.nan) > 0.6)\n",
    "    # implausible duration fraction (very long duration relative to period)\n",
    "    period = feat.get(\"period\", np.nan)\n",
    "    duration = feat.get(\"duration\", np.nan)\n",
    "    try:\n",
    "        flags[\"duration_flag\"] = (duration / period) > 0.15\n",
    "    except Exception:\n",
    "        flags[\"duration_flag\"] = False\n",
    "    # variability dominance\n",
    "    flags[\"var_flag\"] = (feat.get(\"psd_peak\", 0.0) > 0.25) and (feat.get(\"psd_entropy\", 999.0) < 6.0)\n",
    "    # moving object heuristic (short, single, very asymmetric event) - placeholder: rely on data shape / cadence externally\n",
    "    flags[\"moving_object_flag\"] = False  # requires event-level analysis; default False here\n",
    "    flags[\"any_flag\"] = any(flags.get(k, False) for k in flags if k.endswith(\"_flag\"))\n",
    "    return flags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c8d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "run_pipeline.py\n",
    "End-to-end runner. Use --demo to run synthetic data that demonstrates planet vs non-planet signals.\n",
    "Use --lightcurves to point to CSV files (time,flux).\n",
    "\"\"\"\n",
    "\n",
    "def simulate_lightcurve(kind=\"planet\", n_points=3000, period=5.0, duration_frac=0.03, depth=0.005, noise=0.001):\n",
    "    rng = np.random.default_rng()\n",
    "    time = np.cumsum(np.full(n_points, 0.02))  # uniform cadence ~0.02 day (~30 min)\n",
    "    flux = rng.normal(1.0, noise, size=n_points)\n",
    "    if kind == \"planet\":\n",
    "        phase = (time % period) / period\n",
    "        in_tr = (phase < duration_frac) | (phase > 1 - duration_frac)\n",
    "        flux[in_tr] -= depth\n",
    "    elif kind == \"ebinary\":\n",
    "        phase = (time % period) / period\n",
    "        in_tr = (phase < duration_frac) | (phase > 1 - duration_frac)\n",
    "        # deeper, asymmetric\n",
    "        flux[in_tr] -= depth * (1.5 + 0.5 * np.sin(2*np.pi*phase))\n",
    "        # secondary\n",
    "        phase2 = ((time + 0.5*period) % period) / period\n",
    "        sec = (phase2 < duration_frac) | (phase2 > 1 - duration_frac)\n",
    "        flux[sec] -= 0.6 * depth\n",
    "    elif kind == \"variable\":\n",
    "        flux += 0.01 * np.sin(2*np.pi*time/period) + 0.003 * np.sin(2*np.pi*time/(0.7*period))\n",
    "    else:\n",
    "        # meteor-like short spike\n",
    "        i = rng.integers(low=100, high=n_points-100)\n",
    "        flux[i:i+2] -= 0.1  # sharp dip\n",
    "    return time, flux\n",
    "\n",
    "def load_lightcurves_from_globs(patterns):\n",
    "    files = []\n",
    "    for p in patterns:\n",
    "        files.extend(glob.glob(p))\n",
    "    data = []\n",
    "    for fp in files:\n",
    "        try:\n",
    "            df = pd.read_csv(fp)\n",
    "            t, f = prepare_lightcurve_from_df(df)\n",
    "            if len(t) > 0:\n",
    "                data.append((fp, t, f))\n",
    "        except Exception as e:\n",
    "            print(\"Failed to load\", fp, \":\", e)\n",
    "    return data\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--demo\", action=\"store_true\", help=\"Run synthetic demo\")\n",
    "    ap.add_argument(\"--lightcurves\", nargs=\"*\", help=\"Glob patterns to CSV files (time,flux[,flux_err])\")\n",
    "    ap.add_argument(\"--outdir\", default=\"artifacts\", help=\"Save outputs here\")\n",
    "    args = ap.parse_args()\n",
    "    os.makedirs(args.outdir, exist_ok=True)\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    meta = []\n",
    "\n",
    "    if args.demo:\n",
    "        kinds = [\"planet\",\"planet\",\"ebinary\",\"variable\",\"planet\",\"ebinary\",\"variable\",\"planet\",\"meteor\"]\n",
    "        for k in kinds:\n",
    "            t, f = simulate_lightcurve(kind=k)\n",
    "            feat = extract_features(t, f-1.0)  # features expect relative flux near 0\n",
    "            row = [feat.get(c, np.nan) for c in FEATURE_COLUMNS]\n",
    "            X.append(row)\n",
    "            y.append(1 if k == \"planet\" else 0)\n",
    "            meta.append({\"kind\": k, \"feat\": feat, \"vet\": vet_flags(feat)})\n",
    "    elif args.lightcurves:\n",
    "        data = load_lightcurves_from_globs(args.lightcurves)\n",
    "        for fp, t, f in data:\n",
    "            feat = extract_features(t, f)\n",
    "            row = [feat.get(c, np.nan) for c in FEATURE_COLUMNS]\n",
    "            X.append(row)\n",
    "            # placeholder label: unknown; team should label or use provided training labels\n",
    "            y.append(0)\n",
    "            meta.append({\"file\": fp, \"feat\": feat, \"vet\": vet_flags(feat)})\n",
    "    else:\n",
    "        ap.print_help()\n",
    "        return\n",
    "\n",
    "    X = np.array(X, dtype=float)\n",
    "    y = np.array(y, dtype=int)\n",
    "\n",
    "    # Baseline model\n",
    "    model = build_baseline_model()\n",
    "    # Safe: if all labels equal, skip fit\n",
    "    if len(np.unique(y)) > 1:\n",
    "        model.fit(X, y)\n",
    "        proba = model.predict_proba(X)[:,1]\n",
    "    else:\n",
    "        # trivial case for demo: use depth threshold\n",
    "        proba = np.array([0.8 if (row[2] > 0.001) else 0.2 for row in X])  # depth index in FEATURE_COLUMNS\n",
    "    metrics = evaluate_and_save(y, proba, outdir=args.outdir)\n",
    "\n",
    "    # Save per-object predictions and flags\n",
    "    rows = []\n",
    "    for i, m in enumerate(meta):\n",
    "        rows.append({\n",
    "            \"index\": i,\n",
    "            \"pred_proba\": float(proba[i]),\n",
    "            \"label\": int(y[i]),\n",
    "            \"meta\": m\n",
    "        })\n",
    "    with open(os.path.join(args.outdir, \"predictions.json\"), \"w\") as f:\n",
    "        json.dump(rows, f, indent=2)\n",
    "\n",
    "    # Save feature table\n",
    "    feat_df = pd.DataFrame(X, columns=FEATURE_COLUMNS)\n",
    "    feat_df[\"label\"] = y\n",
    "    feat_df[\"pred_proba\"] = proba\n",
    "    feat_df.to_csv(os.path.join(args.outdir, \"features_predictions.csv\"), index=False)\n",
    "\n",
    "    print(\"Done. Outputs saved to:\", args.outdir)\n",
    "    print(\"Metrics:\", json.dumps(metrics, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e9a2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load trained model and feature columns\n",
    "model = joblib.load(\"exoplanet_model.pkl\")\n",
    "feature_columns = joblib.load(\"feature_columns.pkl\")\n",
    "\n",
    "@app.post(\"/predict/\")\n",
    "async def predict(file: UploadFile = File(...), threshold: float = 0.5):\n",
    "    try:\n",
    "        # Load CSV\n",
    "        df = pd.read_csv(file.file)\n",
    "\n",
    "        # Keep only the features used in training\n",
    "        X = df[feature_columns]\n",
    "\n",
    "        # Predict probabilities\n",
    "        probs = model.predict_proba(X)[:, 1]\n",
    "\n",
    "        # Apply threshold\n",
    "        preds = (probs >= threshold).astype(int)\n",
    "\n",
    "        # Return predictions with probability\n",
    "        results = []\n",
    "        for p, prob in zip(preds, probs):\n",
    "            results.append({\n",
    "                \"prediction\": int(p),\n",
    "                \"probability\": float(prob),\n",
    "                \"label\": \"Planet detected\" if p == 1 else \"No planet\"\n",
    "            })\n",
    "\n",
    "        return {\"results\": results}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31653da",
   "metadata": {},
   "outputs": [],
   "source": [
    "For deployment of our application:\n",
    "\n",
    "Run: \n",
    "\n",
    "uvicorn main:app --reload\n",
    "\n",
    "In the bash terminal, and then click on the link:\n",
    "\n",
    "http://127.0.0.1:8000/docs\n",
    "\n",
    "To access the deployed application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe400bd",
   "metadata": {},
   "source": [
    "8: Conclusion\n",
    "\n",
    "Project Completed Successfully!\n",
    "\n",
    "This solution addresses NASA Space Apps Hackathon challenges with global impact."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
